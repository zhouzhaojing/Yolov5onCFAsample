{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459e2c2a-7ed8-4553-8eef-16414566b501",
   "metadata": {},
   "source": [
    "# Container Playground Yolo-V5 full stack sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7601e10a-31c9-42d6-b7db-3060a092091d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### First part: Build your model on DevCloud\n",
    "The first part will be different according to different training frameworks, different algorithms, and different training datasets. For example, building a yolov5 neural network under the pytoch training framework to complete the training of the COCO dataset; building a simple neural network under the pytorch training framework to complete the training of the mnist dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20686873-207c-4728-87bb-58e8f11f8e6f",
   "metadata": {},
   "source": [
    "### Second part: Training on AWS\n",
    "The second part is based on the AWS interface to submit training tasks. First install devcloud_sagemaker provided by AWS. Import three functions from this fileÔºöcreate_training_job, get_training_job_status, get_model„ÄÇThen follow the order of submitting the training task, querying the task status, and downloading the trained model to complete the training and download the model on the AWS platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26aaf76-607e-41b9-a41e-738346829b5f",
   "metadata": {},
   "source": [
    "### Third part: Deploy model on DevCloud Edge Nodes\n",
    "The third part is basically the same as other samples on DevCloud, converting models, deploying models, and observing performance indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2909a09-3bc9-4f82-9309-3987d56b8016",
   "metadata": {},
   "source": [
    "### Related concepts\n",
    "#### YOLOv5 introduction\n",
    "YOLO an acronym for 'You only look once', is an object detection algorithm that divides images into a grid system. Each cell in the grid is responsible for detecting objects within itself.\n",
    "\n",
    "YOLO is one of the most famous object detection algorithms due to its speed and accuracy.\n",
    "\n",
    "Python>=3.6.0 is required with all requirements.txt installed including PyTorch>=1.7:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483069aa-d275-4304-ac6d-9ae137dbad8d",
   "metadata": {},
   "source": [
    "### Build and train the model     \n",
    "\n",
    "#### Download yolov5 \n",
    "Python>=3.6.0 is required with all requirements.txt installed including PyTorch>=1.7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e2e5df-1664-4043-9a84-f6202a7bdc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b0da1-f0d7-47dc-88b4-0a305fb1d5ca",
   "metadata": {},
   "source": [
    "#### Install third packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0f7282-a1ae-42eb-89ca-74a6d93405f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33875f3-bb08-477e-b625-370b25667769",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install  torchvision  torchaudio "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a1c2b8-5843-4632-bdf4-9554ad03376b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train model Locally and save model(optional)\n",
    "\n",
    "Yolov5 maintains its own dataset warehouse. Running the training script will automatically download the dataset for model training. The structure of the dataset is a jpg photo corresponding to a txt label description file.\n",
    "\n",
    "If you want to use your own dataset, you can modify the dataset description yaml file in the data folder„ÄÇYou can also modify the parameters in the train.py script, such as epoch, etc.\n",
    "\n",
    "Run commands below to reproduce results on COCO dataset (dataset auto-downloads on first use). Training times for YOLOv5s/m/l/x are 2/4/6/8 days on a single V100 (multi-GPU times faster). Use the largest --batch-size your GPU allows (batch sizes shown for 16 GB devices)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "49e78080-5b28-4f35-a64a-23c88d2c974a",
   "metadata": {},
   "source": [
    "!python3 yolov5/train.py --data coco128.yaml --cfg yolov5s.yaml --weights '' --batch-size 4 --epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870328bb-c8bf-4f37-bba7-2d4bd15be6ca",
   "metadata": {},
   "source": [
    "### Install devcloud_sagemaker\n",
    "## Training on AWS\n",
    "    \n",
    "1. Install devcloud_sagemaker\n",
    "2. Create a training task\n",
    "3. Query the status of the training task\n",
    "4. Download the trained model from AWS\n",
    "5. Convert the model format \n",
    "6. Run benchmark tests on DevCloud nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6bd829-1cea-40b3-9660-3e0e8a9b42e3",
   "metadata": {},
   "source": [
    "### Install devcloud_sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "888982c6-8f4d-4726-a36c-3e3745211f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting devcloud_sagemaker_user\n",
      "  Downloading devcloud_sagemaker_user-1.8.tar.gz (5.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/pytest/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[?25hCollecting pytest==2.9.2\n",
      "  Downloading pytest-2.9.2-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m324.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/ov2022.1.0-venv/lib/python3.8/site-packages (from devcloud_sagemaker_user) (2.28.1)\n",
      "Collecting py>=1.4.29\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m506.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /opt/ov2022.1.0-venv/lib/python3.8/site-packages (from requests->devcloud_sagemaker_user) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/ov2022.1.0-venv/lib/python3.8/site-packages (from requests->devcloud_sagemaker_user) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/ov2022.1.0-venv/lib/python3.8/site-packages (from requests->devcloud_sagemaker_user) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/ov2022.1.0-venv/lib/python3.8/site-packages (from requests->devcloud_sagemaker_user) (1.26.12)\n",
      "Building wheels for collected packages: devcloud_sagemaker_user\n",
      "  Building wheel for devcloud_sagemaker_user (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for devcloud_sagemaker_user: filename=devcloud_sagemaker_user-1.8-py3-none-any.whl size=5749 sha256=a41f3afd72c14755c4c329e70a65325754c948a68c2eabe50648561d4d50e7c0\n",
      "  Stored in directory: /home/build/.cache/pip/wheels/2d/35/f6/99e52445d0c80670069987e8f72317f59d4d4b2fc6816d740a\n",
      "Successfully built devcloud_sagemaker_user\n",
      "Installing collected packages: py, pytest, devcloud_sagemaker_user\n",
      "Successfully installed devcloud_sagemaker_user-1.8 py-1.11.0 pytest-2.9.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install devcloud_sagemaker_user --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b9850db-5c19-46a6-8fed-78516972590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from devcloud_sagemaker_user.sm_client import *\n",
    "import tarfile, os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3abc459-7183-496b-8d72-949a9a04b04c",
   "metadata": {},
   "source": [
    "## Query the available training device lise and corresponding Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b525803-8259-4e10-af2f-886be2c2e55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instance_type': 'ml.m5.large', 'price_per_hour': 1.114, 'instance_note': '8 GiB of Memory, 2 vCPUs, EBS only, 64-bit platform'}\n",
      "{'instance_type': 'ml.m5.4xlarge', 'price_per_hour': 8.917, 'instance_note': '64 GiB of Memory, 16 vCPUs, EBS only, 64-bit platform'}\n",
      "{'instance_type': 'ml.m5.24xlarge', 'price_per_hour': 53.497, 'instance_note': '384 GiB of Memory, 96 vCPUs, EBS only, 64-bit platform'}\n",
      "{'instance_type': 'ml.m4.2xlarge', 'price_per_hour': 6.186, 'instance_note': '32 GiB of memory, 8 vCPUs, EBS-only, 64-bit platform'}\n",
      "{'instance_type': 'ml.m5.xlarge', 'price_per_hour': 2.229, 'instance_note': '16 GiB of Memory, 4 vCPUs, EBS only, 64-bit platform'}\n",
      "{'instance_type': 'ml.c4.8xlarge', 'price_per_hour': 19.955, 'instance_note': '60 GiB of memory, 36 vCPUs, 64-bit platform'}\n",
      "{'instance_type': 'ml.c5.9xlarge', 'price_per_hour': 14.64, 'instance_note': '72 GiB of memory, 36 vCPUs, 64-bit platform'}\n",
      "{'instance_type': 'ml.c4.2xlarge', 'price_per_hour': 4.989, 'instance_note': '15 GiB of memory, 8 vCPUs, 64-bit platform'}\n",
      "{'instance_type': 'ml.m5.12xlarge', 'price_per_hour': 126.749, 'instance_note': '192 GiB of Memory, 48 vCPUs, EBS only, 64-bit platform'}\n",
      "{'instance_type': 'ml.m4.4xlarge', 'price_per_hour': 12.373, 'instance_note': '64 GiB of memory, 16 vCPUs, EBS-only, 64-bit platform'}\n",
      "{'instance_type': 'ml.c4.4xlarge', 'price_per_hour': 9.978, 'instance_note': '30 GiB of memory, 16 vCPUs, 64-bit platform'}\n",
      "{'instance_type': 'mml.c5.2xlarge', 'price_per_hour': 3.253, 'instance_note': '16 GiB of memory, 8 vCPUs, 64-bit platform'}\n",
      "{'instance_type': 'ml.c5.4xlarge', 'price_per_hour': 6.507, 'instance_note': '32 GiB of memory, 16 vCPUs, 64-bit platform'}\n",
      "{'instance_type': 'ml.m4.16xlarge', 'price_per_hour': 49.495, 'instance_note': '256 GiB of memory, 64 vCPUs, EBS-only, 64-bit platform'}\n",
      "{'instance_type': 'ml.c5.18xlarge', 'price_per_hour': 29.279, 'instance_note': '144 GiB of memory, 72 vCPUs, 64-bit platform'}\n",
      "{'instance_type': 'ml.m4.xlarge', 'price_per_hour': 3.097, 'instance_note': '16 GiB of memory, 4 vCPUs, EBS-only, 64-bit platform'}\n",
      "{'instance_type': 'ml.m4.10xlarge', 'price_per_hour': 30.933, 'instance_note': '160 GiB of memory, 40 vCPUs, EBS-only, 64-bit platform'}\n",
      "{'instance_type': 'ml.c5.xlarge', 'price_per_hour': 1.627, 'instance_note': '8 GiB of memory, 4 vCPUs, 64-bit platform'}\n",
      "{'instance_type': 'ml.c4.xlarge', 'price_per_hour': 2.495, 'instance_note': '3.75 GiB of memory, 2 vCPUs, 64-bit platform'}\n"
     ]
    }
   ],
   "source": [
    "query_device_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be0c91-0506-4198-8cd6-51f52f2b6220",
   "metadata": {},
   "source": [
    "### Creating a training task\n",
    "The submitted parameter contains 4 parts:\n",
    "- Training dataset. It can be the local dataset folder or the URL of the dataset S3 bucket. Using the s3 bucket link can avoid failure of training task due to failure to upload local datasets\n",
    "- Packaged source code. It will be packaged before submitting the source code.And it should be noted that train.py is the startup file in the source code folder\n",
    "- Pre-trained model. In this example, the value is yolov5s.pt.\n",
    "- Training parameters, including two parts:\n",
    "    - Define device type. including instance_type instance_count framework_version\n",
    "    - Define training hyperparameters. The hyperparameters must be the content of the parameter in train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d52c36-57a3-4602-99e3-40db189480bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(\"yaru\",\"intelpass\") #login to get token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4efef-7706-466f-9b7e-b4384622cefb",
   "metadata": {},
   "source": [
    "#### **Manually modify the source code folder yolov5**\n",
    "Before packaging the source code, we need to make some modifications to the cloned yolov5 source code folder.\n",
    "\n",
    "1. Modify the requirements.txt in the yolov5 directory, where you need to modify the version number of the torch and torchvision modules, and then comment out opencv. After the modification, it is as follows:\n",
    "\n",
    "torch==1.9.1              \n",
    "torchvision==0.10.1             \n",
    "#opencv-python>=4.1.2              \n",
    "\n",
    "2. Modify the tran.py file in the yolov5 directory , There are two parts that need to be modified.\n",
    "\n",
    "In line 17, add the followingcontent below **import os**:   \n",
    "\n",
    "os.system('/opt/conda/bin/python3.6 -m pip install -r requirements.txt -i https://opentuna.cn/pypi/web/simple')\n",
    "\n",
    "\n",
    "In line 403, Add the following content below **torch.save(ckpt, best)**:\n",
    "\n",
    "#sagemaker output                  \n",
    "sage_output = os.environ[\"SM_MODEL_DIR\"]                     \n",
    "best_sg = os.path.join(sage_output, 'model.pt')                 \n",
    "torch.save(ckpt, best_sg)                 \n",
    "               \n",
    "\n",
    "In line 460, find the parse_opt function and modify the path, mainly to modify the ROOT path. The modified content is as follows:\n",
    "\n",
    "parser.add_argument('--weights', type=str, default='../input/data/weights/yolov5s.pt', help='initial weights path')          \n",
    "parser.add_argument('--cfg', type=str, default='models/yolov5s.yaml', help='model.yaml path')           \n",
    "parser.add_argument('--data', type=str, default='data/coco.yaml', help='dataset.yaml path')           \n",
    "parser.add_argument('--hyp', type=str, default='data/hyps/hyp.scratch.yaml', help='hyperparameters path')     \n",
    "\n",
    "parser.add_argument('--project', default='runs/train', help='save to project/name')          \n",
    "\n",
    "3. Modify coco.yaml in the yolov5/data directory, and modify line 11 to the following:           \n",
    "path: ../input/data/datasets/coco  # dataset root dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a327839-f2fc-4ea3-8cda-ba5a4cb6c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobid_local = submit_a_task(\"datasets_simple\", \"yolov5.tar\", \"yolov5s.pt\", \n",
    "                    {\"instance_type\":\"ml.p3.2xlarge\", \n",
    "                     \"instance_count\":\"1\", \n",
    "                     \"framework_version\":\"1.8.1\", \n",
    "                     \"hyperparameters\":{\"imgsz\":\"640\", \"epochs\":\"10\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d367becc-41d8-4960-9402-b58e1a0d85d0",
   "metadata": {},
   "source": [
    "In addition to uploading the dataset locally, you can also choose to upload it to the S3 bucket first, and then configure the dataset value the address of the bucket. The advantage is that it will save the time of uploading the dataset locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e652d0-1fb1-4b89-b76b-df6cf039f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobid_s3_url = submit_a_task(\"s3://code-devcloud/jobs/05ec9d2d-8862-49c1-9d1c-a39125e23fc2/Traindata/datasets_simple/\",\n",
    "                    \"yolov5.tar\",\n",
    "                    \"yolov5s.pt\",\n",
    "                    {\"instance_type\":\"ml.m4.4xlarge\",\n",
    "                    \"instance_count\":\"1\",\n",
    "                    \"framework_version\":\"1.8.1\",\n",
    "                    \"hyperparameters\":{\"imgsz\":\"640\", \"epochs\":\"10\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f97df1-3683-489e-8286-d22bc46dfe67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "503fc97a-4c6a-48dd-89b3-eba89d26df30",
   "metadata": {},
   "source": [
    "### Query the status of the training task\n",
    "After the query status is complete, you can run the next module to download the model„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c6035-2689-4861-bb4b-90693d3f9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_training_job_status(jobid_local)\n",
    "get_task_status(jobid_local)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e1d531-4aac-44cb-bece-3404daf02d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_task_info(jobid_local)   #inpout task id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafffdd5-828f-4820-a0e3-976682c0d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_task_log(jobid_local,2)    #inpout task id,n umber of printed log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba06f9e-0413-430e-83a3-6374fabfae70",
   "metadata": {},
   "source": [
    "### Download the trained model from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b116266-636d-4680-b8fc-1980770475b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Download trained model from: https://code-devcloud.s3.cn-north-1.amazonaws.com.cn/5d4152c3-b893-44d2-9a76-cfb8fe51270c/output/model.tar.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAXD3YERHOJTDWZEMR%2F20221102%2Fcn-north-1%2Fs3%2Faws4_request&X-Amz-Date=20221102T134325Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Security-Token=FwoDYXdzEO7%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDHPlxAUnj8dPHOEHdCKmAtfbgQQnbboU%2BXpo%2BfNty1M5ZBywodOMBWzu1TdeZAs5cLWL6oizHp4fqCR7p4Wg7SNRq4PpIIZV3kK36CvCf7CbFWu4dN1Cvwr5PWEgj9pAhe1Sm5Fh%2B%2By2bK1b6cIdAQJLcTWuiBXuZzo1fr1Pl%2Ffa2kV6S%2B2I6UPjEfYTKyauzfZUQBMJxXUGk6kot8cSEew8ElgLiBLiWzf3%2FBu95v4cIRDZZ8uhYiCbVwa5QCYChBa033rD7LwUu%2BPnBo%2Fw4DqQi%2Fa3FCeQj4PokKkLJCTE%2BwRYHJzoKDr0N5%2BvCcBylIDFoU5x7Ch3%2F3TU%2FCsCbiFJH%2FLEkt2WvYQK1dvWHvuhywy%2FukF9btyTlKVTczHLWEk%2Bav9UH4eRu%2BBtL%2BtCpEpVNujHjyj654mbBjItd1mJRhhyIP8biYgC6s2wSSQsY29iLJOJWN4KTMEuNXyLtoxoQHnkTaOIwlqy&X-Amz-Signature=0264049389da2a585af71b1473a22eac9e4176de247b0e4026d03c07334750f5\n",
      "Download completed!\n"
     ]
    }
   ],
   "source": [
    "download_trained_model(jobid_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12ae01eb-0106-4c92-87d3-52876bdec035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query completed, account: u80176 info {'account_id': 6.0, 'account_email': 'u80176@intel.com', 'credits': 14.64, 'account_name': 'u80176'}\n"
     ]
    }
   ],
   "source": [
    "query_account_info(os.environ.get('USER'))#input account name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2daa01c-71f0-4492-9f61-cc4880a0a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cancel_task(\"jobid_local\")  #inpout task id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fa4412-e184-4781-97c3-d4c19c5f865d",
   "metadata": {},
   "source": [
    "### First convert downaded model file to onnx format, then to OpenVINO IR file\n",
    "#### convert to onnx format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9885d-ec55-41e0-ab97-1ce23afc517a",
   "metadata": {},
   "source": [
    "Unzip the Yolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd344f-d804-4f1f-9147-7b8744887d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -zxvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8f87808-8955-4afa-991b-0b8d4bb831b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-mt8j15ik because the default path (/home/build/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco.yaml, weights=['model.pt'], imgsz=[640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "YOLOv5 üöÄ v6.2-205-geef9057 Python-3.8.2 torch-1.9.0+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from model.pt with output shape (1, 25200, 85) (55.7 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.12.0...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 3.5s, saved as model.onnx (28.0 MB)\n",
      "\n",
      "Export complete (4.8s)\n",
      "Results saved to \u001b[1m/data\u001b[0m\n",
      "Detect:          python detect.py --weights model.onnx \n",
      "Validate:        python val.py --weights model.onnx \n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'model.onnx')  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "# onnx>=1.9.0  # ONNX export\n",
    "!pip3 install onnx>=1.9.0\n",
    "# onnx-simplifier>=0.3.6  # ONNX simplifier\n",
    "!python3 yolov5/export.py --weights model.pt --data data/coco.yaml --imgsz 640 --batch-size 1 --include onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c35f07-a558-45ad-a0fb-4bc39334d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FP32 IR files\n",
    "!mo \\\n",
    "--input_model model.onnx \\\n",
    "--input_shape [1,3,640,640] \\\n",
    "--data_type FP32 \\\n",
    "--output_dir data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531d353-f5fa-4440-b246-deba6ecd4b1d",
   "metadata": {},
   "source": [
    "## Build docker image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826052a5-cf59-4c1a-bfa5-0bdf07b3eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!buildah bud --format docker -f ./dockerfile/onnx_yolov5.dockerfile -t $REGISTRY_URL/yolov5:custom ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990bcd4-0b81-4650-a218-2879f275cc7b",
   "metadata": {},
   "source": [
    "## Push custom image to Container Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40f3eb-d73e-4f90-994e-e054dec7a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildah push $REGISTRY_URL/yolov5:custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa54a47-e04d-4344-9754-76efbd8f26db",
   "metadata": {},
   "source": [
    "## Please go back to Container Playground's My Library to lunch this container\n",
    "Navigate to **My Library** > **Resources** and associate the ``ovep-object-detection:custom`` resource with a project, configure the **Output Mount Point** with ``/mount_folder`` and **Environment Variables** with required runtime DEVICE value. Finally click on the launch button.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4687cfe6-ade7-43ef-9a2d-22cb06a52e95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenVINO 2022.1.0",
   "language": "python",
   "name": "ov2022.1.0-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
